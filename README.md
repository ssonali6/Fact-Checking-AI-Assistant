# ğŸ§  Deep Research AI Agent

## ğŸ“Œ Overview
This project implements a **Deep Research AI Agentic System** using **Tavily for web crawling**, with **LangChain & LangGraph** for multi-agent processing. The system consists of:

1. **Research Agent** â€“ Collects relevant articles.  
2. **Fact-Checking Agent** â€“ Analyzes credibility and verifies claims.  
3. **Answer Drafting Agent** â€“ Structures a response using LLMs.  

The project is built using **Python, Streamlit, LangChain, LangGraph, and Tavily**.  

---

## ğŸ“Œ Features
âœ… Uses **Tavily API** for online research  
âœ… Leverages **LangGraph & LangChain** for multi-agent workflow  
âœ… Extracts keywords using **spaCy**  
âœ… Fact-checks claims using **Llama3**  
âœ… Generates structured fact-checking reports  

---

## ğŸ“Œ Installation & Setup
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
streamlit run app.py
## ğŸ“Œ Architechture
ğŸ”¹ Multi-Agent System

| ğŸ— **Agent**              | ğŸ” **Function**                                   |
|---------------------------|---------------------------------------------------|
| **Research Agent**        | Collects relevant articles from Tavily & Wikipedia|
| **Fact-Checking Agent**   | Assigns reliability scores & verifies claims      |

ğŸ”¹ Workflow
1ï¸âƒ£ User enters a claim â†’
2ï¸âƒ£ Research Agent collects data â†’
3ï¸âƒ£ Fact-Checking Agent verifies the claim â†’
4ï¸âƒ£ Draft Agent summarizes findings â†’
âœ… Result is displayed in Streamlit with sources & reliability scores.

ğŸ“Œ How to Use
Enter a claim in the Streamlit web app
Press "Enter"
View Fact-Check Results with reliability scores 
Download full report (Markdown)

## ğŸ“Œ Repository Structure  

```bash
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ config.yaml         # Search & API configurations
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # Project documentation
â”œâ”€â”€ .gitignore          # Excludes sensitive files
â””â”€â”€ YourNotebook.ipynb  # Google Colab notebook (if included)

âœ… 1ï¸âƒ£ Step-by-Step Demo Flow

## **ğŸ“Œ Step 1: Open Google Colab & Launch Ollama**  
Since this project requires **Ollama (Llama 3)** for fact-checking, it must be installed and launched before running Streamlit.  

### **â–¶ 1ï¸âƒ£ Open the Google Colab Notebook**  
[Click here to open the Colab notebook](YOUR_NOTEBOOK_LINK_HERE)  

### **â–¶ 2ï¸âƒ£ Install Ollama & Launch It**  
Run these commands inside the Colab notebook:  
```bash
!pip install colab-xterm
%load_ext colabxterm
%xterm

Once the terminal opens, run these commands inside the xterm terminal:
 # curl https://ollama.ai/install.sh | sh
 # ollama serve & ollama pull llama3
 # ollama pull llama3
ğŸ”¹ This installs Ollama and pulls the Llama 3 model for fact-checking.

## **ğŸ“Œ Step 2: Install Requirements & Run All Cells**  

Before starting the Streamlit app, follow these steps:  

### **â–¶ 1ï¸âƒ£ Install Required Dependencies**  
Run this in **Google Colab**:  
```bash
!pip install langchain-ollama tavily-python langgraph langchain langchain-community
!pip install wikipedia-api transformers streamlit pyyaml pyngrok ngrok

### **â–¶ 2ï¸âƒ£ Run All Notebook Cells**  

ğŸ”¹ **Run all the cells** once packages are installed in Google Colab.  
ğŸ”¹ This will **set up all agents, load models, and prepare the system**.  
ğŸ”¹ **Enter your own Tavily API key** when prompted.


ğŸ“Œ Step 3: Start the Streamlit App in Google Colab
After running all cells, start Streamlit:
!streamlit run /content/app.py &>/dev/null&

Then, expose the app using ngrok and don't forget to add the ngrok authorization token:
from pyngrok import ngrok

ngrok.set_auth_token("your_ngrok_auth_token")

public_url = ngrok.connect(addr=8501, proto="http", auth=None)

print("Streamlit app is running at:", public_url)

Finally! Click on the ngrok link generated in output section and open it in your browser.

## ğŸ“Œ Demo Screenshots  

### ğŸ  Streamlit Interface  
![Streamlit Interface]([streamlit_interface.png](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/streamlit_interface.png))  

### ğŸ” Fact-Check Result  
![Fact-Check Result](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/fact_check_result.png)  

### âœ Draft Answer  
![Draft Answer]([draft_answer.png](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/draft_answer.png))  

### âš  Bias Detection & Source Reliability Scores Table
![Bias Detection](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/bias_detection.png)  

### ğŸ“Š Source Reliability Chart  
![Source Reliability]([source_reliability.png](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/source_reliability.png)  

### ğŸ“¥ Downloading the Report  
![Download Report Button]([download_report_button.png](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/download_report_button.png))  

### ğŸ“„ Viewing the Downloaded Report  
![Downloaded Report]([downloaded_report.png](https://github.com/ssonali6/Fact-Checking-AI-Assistant/blob/main/downloaded_report.png))  






